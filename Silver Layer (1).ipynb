{"cells":[{"cell_type":"markdown","source":["# **Silver Layer Transformations**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"61ae0028-e599-473c-bacf-1570fc364632"},{"cell_type":"markdown","source":["#### **Packages**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f305bf3-84d6-4e7a-9a01-83b68aea64bc"},{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","from pyspark.sql import SparkSession, DataFrame\n","from pyspark.sql.functions import (\n","    col, lit, trim, when, substring, year, current_timestamp, current_date,\n","    regexp_replace, split, size, length, lower, upper, isnan, isnull,\n","    datediff, to_date, array_contains, concat_ws, round, coalesce\n",")\n","from pyspark.sql.types import StringType, IntegerType, DoubleType, DateType, BooleanType"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":29,"statement_ids":[29],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:48.0405801Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:48.0415742Z","execution_finish_time":"2025-07-23T09:13:48.3691366Z","parent_msg_id":"143eda48-9080-4908-9364-9087847bd94d"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 29, Finished, Available, Finished)"},"metadata":{}}],"execution_count":27,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5eeaf1a2-44ea-40d4-bcb1-194053afbb36"},{"cell_type":"code","source":["# Ensure Spark handles column names case-insensitively\n","spark.conf.set(\"spark.sql.caseSensitive\", \"false\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":30,"statement_ids":[30],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:48.1731295Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:48.3713403Z","execution_finish_time":"2025-07-23T09:13:48.6986138Z","parent_msg_id":"cd715228-b96a-4b94-9099-7f1141784e89"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 30, Finished, Available, Finished)"},"metadata":{}}],"execution_count":28,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f4da2f0b-5c79-4db3-8a6e-7cbe0028a00d"},{"cell_type":"markdown","source":["#### **User Identity**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2d6abd1-87ca-4636-a8a8-e055c2133097"},{"cell_type":"code","source":["# Add this function in your notebook\n","def user_identity_transformations(spark: SparkSession, df: DataFrame) -> DataFrame:\n","    from pyspark.sql.functions import trim\n","    for col_name in df.columns:\n","        df = df.withColumn(col_name, trim(col(col_name)))\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":31,"statement_ids":[31],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:48.3769285Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:48.7005799Z","execution_finish_time":"2025-07-23T09:13:48.9915605Z","parent_msg_id":"4aa63fad-8483-467d-9dcb-ba69a0581fc1"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 31, Finished, Available, Finished)"},"metadata":{}}],"execution_count":29,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0fddd94-9486-4d51-a6e5-4f6dfc7a0307"},{"cell_type":"markdown","source":["#### **LinkedIn Users**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad112d7d-078c-4a09-8d3d-ebf3176f27a9"},{"cell_type":"code","source":["# 1. LinkedIn Users Table Transformations\n","def linkedin_users_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    valid_df = valid_df.withColumn(\"join_date\", to_date(col(\"join_date\"),\"dd-MM-yyyy\"))\n","    \n","    # Clean and standardize string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Standardize Gender column\n","    valid_df = valid_df.withColumn(\n","        \"Gender\",\n","        when(lower(col(\"Gender\")).isin([\"m\", \"male\", \"man\"]), \"Male\")\n","        .when(lower(col(\"Gender\")).isin([\"f\", \"female\", \"woman\"]), \"Female\")\n","        .otherwise(\"Other\")\n","    )\n","    \n","    # Parse Age column to extract age range start and end\n","    valid_df = valid_df.withColumn(\n","        \"Age_Range_Start\",\n","        when(col(\"Age\").contains(\"â€“\"), \n","             regexp_replace(col(\"Age\"), r\"(\\d+)â€“.*\", \"$1\").cast(IntegerType()))\n","        .when(col(\"Age\").contains(\"-\"), \n","             regexp_replace(col(\"Age\"), r\"(\\d+)-.*\", \"$1\").cast(IntegerType()))\n","        .otherwise(col(\"Age\").cast(IntegerType()))\n","    ).withColumn(\n","        \"Age_Range_End\",\n","        when(col(\"Age\").contains(\"â€“\"), \n","             regexp_replace(col(\"Age\"), r\".*â€“(\\d+)\", \"$1\").cast(IntegerType()))\n","        .when(col(\"Age\").contains(\"-\"), \n","             regexp_replace(col(\"Age\"), r\".*-(\\d+)\", \"$1\").cast(IntegerType()))\n","        .otherwise(col(\"Age\").cast(IntegerType()))\n","    )\n","    \n","    # Create age band classification\n","    valid_df = valid_df.withColumn(\n","        \"Age_Band\",\n","        when(col(\"Age_Range_Start\") < 23, \"Young Professional\")\n","        .when(col(\"Age_Range_Start\") < 28, \"Early Career\")\n","        .when(col(\"Age_Range_Start\") < 35, \"Mid Career\")\n","        .when(col(\"Age_Range_Start\") < 42, \"Senior Professional\")\n","        .otherwise(\"Experienced\")\n","    )\n","    \n","    # Process Skills column - count number of skills\n","    valid_df = valid_df.withColumn(\n","        \"Skills_Array\",\n","        split(col(\"Skills\"), \",\")\n","    ).withColumn(\n","        \"Skills_Count\",\n","        size(col(\"Skills_Array\"))\n","    )\n","    \n","    # Standardize Education Level\n","    valid_df = valid_df.withColumn(\n","        \"Education_Level_Standardized\",\n","        when(lower(col(\"EducationLevel\")).contains(\"bachelor\"), \"Bachelor's\")\n","        .when(lower(col(\"EducationLevel\")).contains(\"master\"), \"Master's\")\n","        .when(lower(col(\"EducationLevel\")).contains(\"diploma\"), \"Diploma\")\n","        .when(lower(col(\"EducationLevel\")).contains(\"phd\"), \"PhD\")\n","        .otherwise(col(\"EducationLevel\"))\n","    )\n","    \n","    # Create connection tier classification\n","    valid_df = valid_df.withColumn(\n","        \"Connection_Tier\",\n","        when(col(\"Connections\") < 100, \"Starter\")\n","        .when(col(\"Connections\") < 500, \"Growing\")\n","        .when(col(\"Connections\") < 900, \"Established\")\n","        .otherwise(\"Influencer\")\n","    )\n","    \n","    # Create follower tier classification\n","    valid_df = valid_df.withColumn(\n","        \"Follower_Tier\",\n","        when(col(\"Followers\") < 1500, \"Emerging\")\n","        .when(col(\"Followers\") < 3000, \"Rising\")\n","        .otherwise(\"Popular\")\n","    )\n","    \n","    # Calculate follower to connection ratio\n","    valid_df = valid_df.withColumn(\n","        \"Follower_Connection_Ratio\",\n","        when(col(\"Connections\") > 0, \n","             round(col(\"Followers\") / col(\"Connections\"), 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    return valid_df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":32,"statement_ids":[32],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:48.7386769Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:48.9936092Z","execution_finish_time":"2025-07-23T09:13:49.3163373Z","parent_msg_id":"fb07665f-c4c1-486a-908a-89db886a8f88"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 32, Finished, Available, Finished)"},"metadata":{}}],"execution_count":30,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4aafdc6f-a97f-434e-9e67-e59eb04f2e27"},{"cell_type":"markdown","source":["#### **Posts**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8bc2a382-c6e0-40e0-9bf4-d0a98815db2d"},{"cell_type":"code","source":["# 2. Posts Table Transformations\n","def posts_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    \n","    # Clean string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Convert DatePosted to date type\n","    valid_df = valid_df.withColumn(\"DatePosted\", to_date(col(\"DatePosted\"),\"dd-MM-yyyy\"))\n","    \n","    # Standardize PostType\n","    valid_df = valid_df.withColumn(\n","        \"Post_Type_Standardized\",\n","        when(lower(col(\"PostType\")).contains(\"blog\"), \"Career Update\")\n","        .when(lower(col(\"PostType\")).contains(\"skill share\"), \"Skill Share\")\n","        .when(lower(col(\"PostType\")).contains(\"achievement\"), \"Achievement\")\n","        .otherwise(col(\"PostType\"))\n","    )\n","    \n","    # Create engagement rate calculation\n","    valid_df = valid_df.withColumn(\n","        \"Engagement_Rate\",\n","        when(col(\"Impressions\") > 0, \n","             round((col(\"Reactions\") + col(\"Comments\") + col(\"Shares\")) / col(\"Impressions\") * 100, 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Create post length classification\n","    valid_df = valid_df.withColumn(\n","        \"Post_Length_Category\",\n","        when(col(\"PostLength\") <= 30, \"Short\")\n","        .when(col(\"PostLength\") <= 65, \"Medium\")\n","        .when(col(\"PostLength\") <= 100, \"Long\")\n","        .otherwise(\"Very Long\")\n","    )\n","    \n","    # Create viral post flag\n","    valid_df = valid_df.withColumn(\n","        \"Is_Viral\",\n","        when((col(\"Reactions\") > 100) | (col(\"Shares\") > 50), True)\n","        .otherwise(False)\n","    )\n","    \n","    # Create high engagement flag\n","    valid_df = valid_df.withColumn(\n","        \"High_Engagement\",\n","        when(col(\"Engagement_Rate\") > 5, True)\n","        .otherwise(False)\n","    )\n","    \n","    # Standardize AttachmentType\n","    valid_df = valid_df.withColumn(\n","        \"Attachment_Type_Standardized\",\n","        when(lower(col(\"AttachmentType\")).contains(\"text\"), \"Text\")\n","        .when(lower(col(\"AttachmentType\")).contains(\"image\"), \"Image\")\n","        .when(lower(col(\"AttachmentType\")).contains(\"video\"), \"Video\")\n","        .when(lower(col(\"AttachmentType\")).contains(\"link\"), \"Link\")\n","        .otherwise(\"Other\")\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    \n","    return valid_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:49.2919897Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:49.3189937Z","execution_finish_time":"2025-07-23T09:13:49.6293222Z","parent_msg_id":"bdf06b94-4eda-40e9-95fe-55eaa42a9d81"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 33, Finished, Available, Finished)"},"metadata":{}}],"execution_count":31,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0afcfb72-7ae6-43ba-998e-ec7f4c3fe5e8"},{"cell_type":"markdown","source":["#### **Connection Growth**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c9a61c9-3b44-4c4c-ab5c-3d88f6901f00"},{"cell_type":"code","source":["# 3. Connection Growth Table Transformations\n","def connection_growth_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    \n","    # Clean string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Convert Date to date type\n","    valid_df = valid_df.withColumn(\"Date\", to_date(col(\"Date\"),\"dd-MM-yyyy\"))\n","    \n","    # Calculate acceptance rate\n","    valid_df = valid_df.withColumn(\n","        \"Invite_Acceptance_Rate\",\n","        when(col(\"InvitesSent\") > 0, \n","             round(col(\"InvitesAccepted\") / col(\"InvitesSent\") * 100, 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Create growth category\n","    valid_df = valid_df.withColumn(\n","        \"Growth_Category\",\n","        when(col(\"ConnectionGrowthRate\") < 0, \"Declining\")\n","        .when(col(\"ConnectionGrowthRate\") == 0, \"Stagnant\")\n","        .when(col(\"ConnectionGrowthRate\") < 15, \"Slow Growth\")\n","        .when(col(\"ConnectionGrowthRate\") < 40, \"Moderate Growth\")\n","        .otherwise(\"High Growth\")\n","    )\n","    \n","    # Create activity level classification\n","    valid_df = valid_df.withColumn(\n","        \"Activity_Level\",\n","        when((col(\"MessagesSent\") > 10) | (col(\"InvitesSent\") > 20), \"High\")\n","        .when((col(\"MessagesSent\") > 5) | (col(\"InvitesSent\") > 10), \"Medium\")\n","        .otherwise(\"Low\")\n","    )\n","    \n","    # Create engagement score\n","    valid_df = valid_df.withColumn(\n","        \"Engagement_Score\",\n","        col(\"MessagesSent\") + col(\"InvitesSent\") + col(\"GroupsJoined\") + (col(\"ProfileViews\") / 10)\n","    )\n","    \n","    # Flag for network expansion\n","    valid_df = valid_df.withColumn(\n","        \"Network_Expanding\",\n","        when(col(\"NewConnections\") > 0, True)\n","        .otherwise(False)\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    \n","    return valid_df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":34,"statement_ids":[34],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:49.6630155Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:49.6642143Z","execution_finish_time":"2025-07-23T09:13:49.9708185Z","parent_msg_id":"4ae31134-ebee-4eb6-97b7-b4e78f189fac"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 34, Finished, Available, Finished)"},"metadata":{}}],"execution_count":32,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ded5b3f0-a438-4bbf-951d-8c705263123b"},{"cell_type":"markdown","source":["#### **Post Performance**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c711825-c8cd-454c-b3c2-698160b750c9"},{"cell_type":"code","source":["# 4. Post Performance Table Transformations\n","def post_performance_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    \n","    # Clean string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Recalculate engagement rate for consistency\n","    valid_df = valid_df.withColumn(\n","        \"Calculated_Engagement_Rate\",\n","        when(col(\"Impressions\") > 0, \n","             round((col(\"Reactions\") + col(\"Comments\") + col(\"Shares\")) / col(\"Impressions\") * 100, 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Calculate click-through rate percentage\n","    valid_df = valid_df.withColumn(\n","        \"CTR_Percentage\",\n","        when(col(\"Impressions\") > 0, \n","             round(col(\"Clicks\") / col(\"Impressions\") * 100, 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Create performance tier\n","    valid_df = valid_df.withColumn(\n","        \"Performance_Tier\",\n","        when(col(\"Calculated_Engagement_Rate\") > 10, \"Excellent\")\n","        .when(col(\"Calculated_Engagement_Rate\") > 5, \"Good\")\n","        .when(col(\"Calculated_Engagement_Rate\") > 2, \"Average\")\n","        .otherwise(\"Poor\")\n","    )\n","    \n","    # Standardize ContentType\n","    valid_df = valid_df.withColumn(\n","        \"Content_Type_Standardized\",\n","        when(lower(col(\"ContentType\")).contains(\"text\"), \"Text\")\n","        .when(lower(col(\"ContentType\")).contains(\"image\"), \"Image\")\n","        .when(lower(col(\"ContentType\")).contains(\"video\"), \"Video\")\n","        .when(lower(col(\"ContentType\")).contains(\"link\"), \"Link\")\n","    )\n","    \n","    # Create viral content flag\n","    valid_df = valid_df.withColumn(\n","        \"Is_Viral_Content\",\n","        when((col(\"Reactions\") > 400) | (col(\"Shares\") > 40) | (col(\"Reach\") > 3500), True)\n","        .otherwise(False)\n","    )\n","    \n","    # Calculate reach rate\n","    valid_df = valid_df.withColumn(\n","        \"Reach_Rate\",\n","        when(col(\"Impressions\") > 0, \n","             round(col(\"Reach\") / col(\"Impressions\") * 100, 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    \n","    return valid_df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":35,"statement_ids":[35],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:50.2487584Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:50.2500674Z","execution_finish_time":"2025-07-23T09:13:50.548856Z","parent_msg_id":"b8d35c44-e671-4e47-bb11-3fab50542fbe"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 35, Finished, Available, Finished)"},"metadata":{}}],"execution_count":33,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc0f4410-4433-40cc-8993-b37bcb879e44"},{"cell_type":"markdown","source":["#### **Job Applications**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd0022d9-0b43-4c65-9eca-9eaa6613963b"},{"cell_type":"code","source":["# 5. Job Applications Table Transformations\n","def job_applications_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    \n","    # Clean string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Convert ApplicationDate to date type\n","    valid_df = valid_df.withColumn(\"ApplicationDate\", to_date(col(\"ApplicationDate\"),\"dd-MM-yyyy\"))\n","    \n","    # Standardize Status\n","    valid_df = valid_df.withColumn(\n","        \"Status_Standardized\",\n","        when(lower(col(\"Status\")).contains(\"submit\"), \"Submitted\")\n","        .when(lower(col(\"Status\")).contains(\"interview\"), \"Interview\")\n","        .when(lower(col(\"Status\")).contains(\"select\"), \"Selected\")\n","        .when(lower(col(\"Status\")).contains(\"reject\"), \"Rejected\")\n","        .otherwise(col(\"Status\"))\n","    )\n","    \n","    # Create application outcome flags\n","    valid_df = valid_df.withColumn(\n","        \"Is_Successful\",\n","        when(lower(col(\"Status_Standardized\")) == \"selected\", True)\n","        .otherwise(False)\n","    ).withColumn(\n","        \"Is_In_Progress\",\n","        when(lower(col(\"Status_Standardized\")).isin([\"submitted\", \"interview\"]), True)\n","        .otherwise(False)\n","    )\n","    \n","    # Create referral flag\n","    valid_df = valid_df.withColumn(\n","        \"Used_Referral\",\n","        when(lower(col(\"ReferralUsed\")).isin([\"yes\", \"true\", \"1\"]), True)\n","        .otherwise(False)\n","    )\n","    \n","    # Parse experience required\n","    valid_df = valid_df.withColumn(\n","        \"Experience_Years\",\n","        regexp_replace(col(\"ExperienceRequired\"), r\"[^\\d]\", \"\").cast(IntegerType())\n","    )\n","    \n","    # Create experience category\n","    valid_df = valid_df.withColumn(\n","        \"Experience_Category\",\n","        when(col(\"Experience_Years\") == 0, \"Entry Level\")\n","        .when(col(\"Experience_Years\") < 3, \"Junior\")\n","        .when(col(\"Experience_Years\") < 6, \"Mid Level\")\n","        .when(col(\"Experience_Years\") < 9, \"Senior\")\n","        .otherwise(\"Expert\")\n","    )\n","    \n","    # Calculate days since application\n","    valid_df = valid_df.withColumn(\n","        \"Days_Since_Application\",\n","        datediff(current_date(), col(\"ApplicationDate\"))\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    \n","    return valid_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":36,"statement_ids":[36],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:50.9401706Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:50.9414007Z","execution_finish_time":"2025-07-23T09:13:51.2338907Z","parent_msg_id":"7fc35ae1-2a59-41a4-b1d0-c5afb400d6cb"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 36, Finished, Available, Finished)"},"metadata":{}}],"execution_count":34,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d788c2a8-a6f9-430f-89b1-41af8c727c1f"},{"cell_type":"markdown","source":["#### **User Activity**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc452aca-8f6e-4ee1-876f-9169d9092485"},{"cell_type":"code","source":["# 6. User Activity Table Transformations\n","def user_activity_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    \n","    # Clean string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Convert Date to date type\n","    valid_df = valid_df.withColumn(\"Date\", to_date(col(\"Date\"),\"dd-MM-yyyy\"))\n","    \n","    # Create total activity score\n","    valid_df = valid_df.withColumn(\n","        \"Total_Activity_Score\",\n","        col(\"Logins\") + col(\"MessagesSent\") + col(\"MessagesReceived\") + \n","        col(\"JobsViewed\") + col(\"ProfilesViewed\") + col(\"PostsCreated\") + \n","        col(\"GroupsJoined\") + col(\"NotificationsViewed\")\n","    )\n","    \n","    # Create activity level classification\n","    valid_df = valid_df.withColumn(\n","        \"Activity_Level\",\n","        when(col(\"Total_Activity_Score\") > 50, \"Very High\")\n","        .when(col(\"Total_Activity_Score\") > 35, \"High\")\n","        .when(col(\"Total_Activity_Score\") > 20, \"Medium\")\n","        .when(col(\"Total_Activity_Score\") > 10, \"Low\")\n","        .otherwise(\"Very Low\")\n","    )\n","    \n","    # Create engagement ratios\n","    valid_df = valid_df.withColumn(\n","        \"Message_Response_Rate\",\n","        when(col(\"MessagesSent\") > 0, \n","             round(col(\"MessagesReceived\") / col(\"MessagesSent\"), 2))\n","        .otherwise(0)\n","    )\n","    \n","    # Create job seeker flag\n","    valid_df = valid_df.withColumn(\n","        \"Is_Job_Seeker\",\n","        when(col(\"JobsViewed\") > 7, True)\n","        .otherwise(False)\n","    )\n","    \n","    # Create social networker flag\n","    valid_df = valid_df.withColumn(\n","        \"Is_Social_Networker\",\n","        when((col(\"ProfilesViewed\") > 7) | (col(\"MessagesSent\") > 7), True)\n","        .otherwise(False)\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    \n","    return valid_df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:51.4306284Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:51.4318461Z","execution_finish_time":"2025-07-23T09:13:51.804765Z","parent_msg_id":"d6c232f3-711a-4530-bd27-762a6208ffc1"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 37, Finished, Available, Finished)"},"metadata":{}}],"execution_count":35,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9067d086-45a3-4914-a191-8fffff69e79b"},{"cell_type":"markdown","source":["#### **Company Affiliation**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04b61689-ab2b-41bc-be2c-dfe3f6a98810"},{"cell_type":"code","source":["# 7. Company Affiliation Table Transformations\n","def company_affiliation_transformations(spark: SparkSession, valid_df: DataFrame) -> DataFrame:\n","    \n","    # Clean string columns\n","    for col_name in valid_df.columns:\n","        if valid_df.schema[col_name].dataType == StringType():\n","            valid_df = valid_df.withColumn(col_name, trim(col(col_name)))\n","    \n","    # Convert date columns to date type\n","    valid_df = valid_df.withColumn(\"StartDate\", to_date(col(\"StartDate\"),\"dd-MM-yyyy\"))\n","    valid_df = valid_df.withColumn(\"EndDate\", to_date(col(\"EndDate\"),\"dd-MM-yyyy\"))\n","    \n","    # Create current employment flag\n","    valid_df = valid_df.withColumn(\n","        \"Is_Current_Employment\",\n","        when(col(\"EndDate\").isNull(), True)\n","        .otherwise(False)\n","    )\n","    \n","    # Calculate employment duration\n","    valid_df = valid_df.withColumn(\n","        \"Employment_Duration_Days\",\n","        when(col(\"EndDate\").isNull(), \n","             datediff(current_date(), col(\"StartDate\")))\n","        .otherwise(datediff(col(\"EndDate\"), col(\"StartDate\")))\n","    )\n","    \n","    # Convert duration to years\n","    valid_df = valid_df.withColumn(\n","        \"Employment_Duration_Years\",\n","        round(col(\"Employment_Duration_Days\") / 365.25, 1)\n","    )\n","    \n","    # Create tenure category\n","    valid_df = valid_df.withColumn(\n","        \"Tenure_Category\",\n","        when(col(\"Employment_Duration_Years\") < 3, \"Short\")\n","        .when(col(\"Employment_Duration_Years\") < 6, \"Medium\")\n","        .when(col(\"Employment_Duration_Years\") < 10, \"Long \")\n","    )\n","    \n","    # Standardize Employment Type\n","    valid_df = valid_df.withColumn(\n","        \"Employment_Type_Standardized\",\n","        when(lower(col(\"EmploymentType\")).contains(\"full\"), \"Full-time\")\n","        .when(lower(col(\"EmploymentType\")).contains(\"contract\"), \"Contract\")\n","        .when(lower(col(\"EmploymentType\")).contains(\"intern\"), \"Internship\")\n","        .otherwise(col(\"EmploymentType\"))\n","    )\n","    \n","    # Parse salary range\n","    valid_df = valid_df.withColumn(\n","        \"Salary_Range_Lower\",\n","        when(col(\"SalaryRange\").contains(\"-\"), \n","             regexp_replace(col(\"SalaryRange\"), r\"[^\\d]\", \"\").cast(IntegerType()))\n","        .otherwise(None)\n","    )\n","    \n","    # Create salary category\n","    valid_df = valid_df.withColumn(\n","        \"Salary_Category\",\n","        when(col(\"Salary_Range_Lower\") < 400000, \"Entry Level\")\n","        .when(col(\"Salary_Range_Lower\") < 700000, \"Mid Level\")\n","        .when(col(\"Salary_Range_Lower\") < 100000, \"Senior Level\")\n","        .otherwise(\"Executive Level\")\n","    )\n","    \n","    # Convert Src_Transaction_Date to date type\n","    valid_df = valid_df.withColumn(\"Src_Transaction_Date\", to_date(col(\"Src_Transaction_Date\"),\"dd-MM-yyyy\"))\n","    \n","    return valid_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:51.5791005Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:51.8066676Z","execution_finish_time":"2025-07-23T09:13:52.0883432Z","parent_msg_id":"c7723f7c-2505-4b8a-96fe-eba7bf896130"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 38, Finished, Available, Finished)"},"metadata":{}}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4312d94b-5f9a-4552-bacd-9dc4facdecf7"},{"cell_type":"markdown","source":["#### **Reprocessing Layer**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd632525-a9fb-49e6-b047-0fd0298490d6"},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType\n","\n","# Configuration metadata for silver layer\n","conf_data = [\n","    (\"user_identity\",        False, \"UserID\",     None,     None,            1),\n","    (\"linkedin_users\",       True,  \"UserID\",     \"UserID\",     \"user_identity\", 2),\n","    (\"post\",                 True,  \"PostID\",        \"UserID\",  \"linkedin_users\",3),\n","    (\"connection_growth\",    True,  \"RecordID\",       \"UserID\", \"linkedin_users\",4),\n","    (\"post_performance\",     True,  \"PostID\",        \"PostID\" , \"post\",          5),\n","    (\"job_applications\",     True,  \"ApplicationID\",  \"UserID\", \"linkedin_users\",6),\n","    (\"user_activity\",        True,  \"RecordID\",       \"UserID\", \"linkedin_users\",7),\n","    (\"company_affiliation\",  True,  \"AffiliationID\",  \"UserID\", \"linkedin_users\",8),\n","]\n","\n","schema = StructType([\n","    StructField(\"table_name\", StringType(), True),\n","    StructField(\"requires_reprocessing\", BooleanType(), True),\n","    StructField(\"business_key\", StringType(), True),\n","    StructField(\"comparison_key\", StringType(), True),\n","    StructField(\"comparison\", StringType(), True),\n","    StructField(\"order\", IntegerType(), True)\n","])\n","\n","conf_silver_df = spark.createDataFrame(conf_data, schema)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":39,"statement_ids":[39],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:51.8310419Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:52.0904518Z","execution_finish_time":"2025-07-23T09:13:52.3886894Z","parent_msg_id":"fda145f2-62c0-4c26-ab49-34ca104011ab"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 39, Finished, Available, Finished)"},"metadata":{}}],"execution_count":37,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb14306a-1671-4f8c-ba9b-353b8da42a52"},{"cell_type":"code","source":["# Transformation function mapping\n","transformation_mapping = {\n","    \"user_identity\": user_identity_transformations,\n","    \"linkedin_users\": linkedin_users_transformations,\n","    \"post\": posts_transformations,\n","    \"connection_growth\": connection_growth_transformations,\n","    \"post_performance\": post_performance_transformations,\n","    \"job_applications\": job_applications_transformations,\n","    \"user_activity\": user_activity_transformations,\n","    \"company_affiliation\": company_affiliation_transformations,\n","}\n","\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import col, row_number\n","from pyspark.sql.window import Window\n","\n","def save_all_transformed_tables(spark: SparkSession):\n","    silver_base_path = \"abfss://LinkedIN@onelake.dfs.fabric.microsoft.com/LinkedIN.Lakehouse/Tables/silver/\"\n","    invalid_base_path = \"abfss://LinkedIN@onelake.dfs.fabric.microsoft.com/LinkedIN.Lakehouse/Tables/reproc/\"\n","\n","    # Step 1: Preload comparison tables (only once)\n","    comparison_tables = {}\n","    for row in conf_data:\n","        table_name, _, _, comparison_key, comparison_table, _ = row\n","        if comparison_table and comparison_table not in comparison_tables:\n","            comparison_path = f\"{silver_base_path}{comparison_table}\"\n","            try:\n","                ref_df = spark.read.format(\"delta\").load(comparison_path).select(comparison_key).distinct()\n","                comparison_tables[comparison_table] = ref_df\n","            except Exception as e:\n","                print(f\"âš ï¸ Skipping comparison load for {comparison_table}: {str(e)}\")\n","\n","    # Step 2: Loop through all tables based on conf\n","    for row in conf_data:\n","        table_name, requires_reprocessing, business_key, comparison_key, comparison_table, _ = row\n","\n","        raw_path = f\"abfss://LinkedIN@onelake.dfs.fabric.microsoft.com/LinkedIN.Lakehouse/Tables/BRONZE/{table_name}\"\n","        silver_path = f\"{silver_base_path}{table_name}\"\n","        invalid_path = f\"{invalid_base_path}{table_name}\"\n","\n","        print(f\"\\nðŸ”„ Processing table: {table_name}\")\n","\n","        raw_df = spark.read.format(\"delta\").load(raw_path)\n","        transform_func = transformation_mapping.get(table_name)\n","        transformed_df = transform_func(spark, raw_df) if transform_func else raw_df\n","\n","        # Initialize valid & invalid\n","        valid_df = transformed_df\n","        invalid_df = spark.createDataFrame([], transformed_df.schema)\n","\n","        # Step 3: Comparison check using comparison_key\n","        if comparison_table and comparison_table in comparison_tables:\n","            ref_df = comparison_tables[comparison_table]\n","            missing_keys_df = transformed_df.join(ref_df, on=comparison_key, how=\"left_anti\")\n","            valid_df = transformed_df.join(ref_df, on=comparison_key, how=\"inner\")\n","            invalid_df = invalid_df.unionByName(missing_keys_df)\n","\n","        # Step 4: Reprocessing logic\n","        if requires_reprocessing:\n","            cols = transformed_df.columns\n","\n","            if \"Src_Transaction_Date\" in cols and business_key in cols:\n","                valid_records = valid_df.filter(col(business_key).isNotNull() & col(\"Src_Transaction_Date\").isNotNull())\n","                null_records = valid_df.filter(col(business_key).isNull() | col(\"Src_Transaction_Date\").isNull())\n","            elif business_key in cols:\n","                valid_records = valid_df.filter(col(business_key).isNotNull())\n","                null_records = valid_df.filter(col(business_key).isNull())\n","            else:\n","                valid_records = valid_df\n","                null_records = spark.createDataFrame([], transformed_df.schema)\n","\n","            # Add to invalids\n","            invalid_df = invalid_df.unionByName(null_records)\n","\n","            # Deduplicate if needed\n","            if business_key in cols and \"Src_Transaction_Date\" in cols:\n","                window_spec = Window.partitionBy(business_key).orderBy(col(\"Src_Transaction_Date\").desc())\n","                deduped_df = valid_records.withColumn(\"row_num\", row_number().over(window_spec)) \\\n","                                          .filter(\"row_num = 1\") \\\n","                                          .drop(\"row_num\")\n","            else:\n","                 deduped_df = valid_records.dropDuplicates([business_key])\n","\n","            # Merge to silver\n","            if DeltaTable.isDeltaTable(spark, silver_path):\n","                delta_table = DeltaTable.forPath(spark, silver_path)\n","                delta_table.alias(\"target\").merge(\n","                    deduped_df.alias(\"source\"),\n","                    f\"target.{business_key} = source.{business_key}\"\n","                ).whenMatchedUpdateAll() \\\n","                 .whenNotMatchedInsertAll() \\\n","                 .execute()\n","            else:\n","                deduped_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n","\n","            print(f\"âœ… Valid data written to silver: {table_name}\")\n","        else:\n","            # For static tables like user_identity\n","            valid_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n","            print(f\"âœ… Static table written to silver: {table_name}\")\n","\n","        # Step 5: Save invalid records\n","        if not invalid_df.rdd.isEmpty():\n","            invalid_df.write.format(\"delta\").mode(\"overwrite\").save(invalid_path)\n","            print(f\"âš ï¸ Invalid data written to reproc: {table_name}\")\n","        else:\n","            print(f\"âœ… No invalid data present for table: {table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":40,"statement_ids":[40],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:13:52.1231824Z","session_start_time":null,"execution_start_time":"2025-07-23T09:13:52.3907558Z","execution_finish_time":"2025-07-23T09:13:52.7340356Z","parent_msg_id":"7bc71867-8aa5-43df-8c03-d6a90ee8b37f"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 40, Finished, Available, Finished)"},"metadata":{}}],"execution_count":38,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a618447-1e41-403a-aebf-6cc849d6c8c2"},{"cell_type":"code","source":["save_all_transformed_tables(spark)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":42,"statement_ids":[42],"state":"finished","livy_statement_state":"available","session_id":"c49d8603-0c98-49e6-bb92-e22e62fd3fff","normalized_state":"finished","queued_time":"2025-07-23T09:18:42.3529056Z","session_start_time":null,"execution_start_time":"2025-07-23T09:18:42.3540769Z","execution_finish_time":"2025-07-23T09:19:25.9246102Z","parent_msg_id":"3c1a992d-cea2-4dd6-b1fb-de6a10c8458f"},"text/plain":"StatementMeta(, c49d8603-0c98-49e6-bb92-e22e62fd3fff, 42, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nðŸ”„ Processing table: user_identity\nâœ… Static table written to silver: user_identity\nâœ… No invalid data present for table: user_identity\n\nðŸ”„ Processing table: linkedin_users\nâœ… Valid data written to silver: linkedin_users\nâš ï¸ Invalid data written to reproc: linkedin_users\n\nðŸ”„ Processing table: post\nâœ… Valid data written to silver: post\nâœ… No invalid data present for table: post\n\nðŸ”„ Processing table: connection_growth\nâœ… Valid data written to silver: connection_growth\nâœ… No invalid data present for table: connection_growth\n\nðŸ”„ Processing table: post_performance\nâœ… Valid data written to silver: post_performance\nâœ… No invalid data present for table: post_performance\n\nðŸ”„ Processing table: job_applications\nâœ… Valid data written to silver: job_applications\nâœ… No invalid data present for table: job_applications\n\nðŸ”„ Processing table: user_activity\nâœ… Valid data written to silver: user_activity\nâœ… No invalid data present for table: user_activity\n\nðŸ”„ Processing table: company_affiliation\nâœ… Valid data written to silver: company_affiliation\nâœ… No invalid data present for table: company_affiliation\n"]}],"execution_count":40,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"16c99361-1674-40b3-99a3-5227a9189136"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"ed75a8c9-edde-46ad-9866-bfbeacbedc3c","known_lakehouses":[{"id":"ed75a8c9-edde-46ad-9866-bfbeacbedc3c"}],"default_lakehouse_name":"LinkedIN","default_lakehouse_workspace_id":"962a29d3-89c6-4de4-91d8-6d5fb894ae73"}}},"nbformat":4,"nbformat_minor":5}